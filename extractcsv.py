# -*- coding: utf-8 -*-
"""ExtractCSV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__1lXc5PUYVx1K13r4iapyc93RWj9zLF
"""

import csv # import the csv module for reading and extracting data from the desired csv file

from google.colab import drive # mount drive
drive.mount('/content/gdrive')

import os, sys # for navigating through directories

os.chdir('gdrive/My Drive/DeepOrphan')

os.chdir('LigandReceptorDatabase')

# d,target_gene_symbol,target_uniprot,target_ensembl_gene_id,target_ligand,inputfile = open('interactions.csv','r') # just checking what's in the csv file

# for row in inputfile:
#     print(row)

inputfile = csv.reader(open('interactions.csv','r')) # display more neatly

for row in inputfile:
    print(row)

inputfile = csv.reader(open('interactions.csv','r')) # display more neatly
# inputfile.readline()
for row in inputfile:
    print(row[0])

inputfile = csv.reader(open('interactions.csv','r')) # display more neatly
for column in inputfile:
#     print(column[0])
    print(column[1])
#     print(column[12])
    print(column[13])
    print('\n')

# inputfile = csv.reader(open('interactions.csv','r')) # display more neatly
# for row in inputfile:
#     print(row[0])

inputfile = csv.reader(open('interactions.csv','r'))
next(inputfile) # now this skips the 1st row of the excel sheet
for line in inputfile:
      print(line[0])


# with csv.reader(open('interactions.csv','r')) as f:
#     next(f)
#     for line in f:
#       print(line)

# with open('interactions.csv', 'r') as f:
#     next(f)
#     for line in f:
#       print(line)
# with open('interactions.csv','r') as f:
#   lines = f.readlines()[1:]

inputfile = csv.reader(open('interactions.csv','r')) # display more neatly

firstline = True
for row in inputfile:
    if firstline:    #skip first line
        firstline = False
        continue
    print(row[3])
    print('\n')

# for row in inputfile:
# #     print(column[0])
#     print(row[3])
#     print('\n')

inputfile = csv.reader(open('interactions.csv','r')) # display more neatly

firstline = True
#i = 0
kws = []
for row in inputfile:
    if firstline:    #skip first line
        firstline = False
        continue
    #print(row[3])
    kws.append(row[3])
    #i += 1
    #print('\n')

print(kws)

len(kws)

try:
  import tflearn
except ImportError:
  !pip3 install -q tflearn
  import tflearn

try:
  import progressbar
except ImportError:
  !pip3 install -q progressbar2
  from progressbar import ProgressBar

get_ipython().magic(u'matplotlib inline')
import matplotlib.pyplot as plt
from urllib.request import Request, urlopen
import numpy as np
from skimage.util import view_as_windows as vaw
import tensorflow as tf
import os, sys
import glob
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.layers.recurrent import lstm
from tflearn.layers.core import fully_connected

def get_uniprot_data(kws, numxs):
    '''Goes to the uniprot website and searches for 
       data with the keyword given. Returns the data 
       found up to limit elements.'''
    
    #kws = [kw, 'NOT+' + kw] '''IgG or not IgG'''
    Protein_data = {}
    #len(kws)
    
            
    for i in range(numxs):
        kw = kws[i]
        #https://www.uniprot.org/uniprot/P18054.fasta
        url1 = 'https://www.uniprot.org/uniprot/'
        url2 = '.fasta'
        query_complete = url1 + kw + url2
        request = Request(query_complete)
        response = urlopen(request)
        data = response.read()
        data = str(data, 'utf-8')
        data = data.split('\n')
        data = data[1:-1]
        Protein_data[str(i)] = list(map(lambda x:x.lower(),data))

    x = Protein_data['0'] + Protein_data['1']
    y = np.zeros([len(x), ])
    y[:len(x)//2] = 1.
        
    return x, y

# def cut_strings(seqs, labels, length, padlen=None):
#     if padlen is None:
#         padlen = int(0.95 * length)

#     x = np.zeros([0, length, 1])
#     y = np.zeros([0, ])
#     count = 0
#     xlen = None
#     bar = progressbar.ProgressBar()

#     for seq in bar(seqs):
#         seq_nums = []
#         for letter in seq:
#             seq_nums.append(max(ord(letter)-97, 0))

#         if len(seq_nums) > length:
#             padded_seq = np.pad(np.asarray(seq_nums), (padlen, padlen),
#                                           'constant', constant_values=22.)
#             cut_seq = vaw(padded_seq, (length, ))
#             y = np.concatenate((y, np.ones([cut_seq.shape[0], ])*labels[count]))
#             x = np.concatenate((x, cut_seq[..., None]))
#             count += 1
#         else:
#             continue

#     print('Used {} proteins.'.format(count))
#     plt.hist(x[x != 22.].flatten(), bins=25)
#     plt.show()

#     return x, y

string_len = 15 # how many amino acids to take for each substring
uniprot_limit = 15 # how many proteins to get for each class from uniprot
# how many amino acids to skip during cutting when moving to next cut.
# if this number is 1, it just moves to the next one.
stride = 1 #'''Defines parameters for looking at the protein'''

# call the get_uniprot_data function to get data and labels
X, Y = get_uniprot_data(kws, uniprot_limit)

# X, Y = cut_strings(X, Y, string_len, stride)

print(X)